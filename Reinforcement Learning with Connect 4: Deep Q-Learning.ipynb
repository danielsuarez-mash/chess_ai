{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":187044,"sourceType":"modelInstanceVersion","modelInstanceId":146675,"modelId":169205}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jollychappies/reinforcement-learning-with-connect-4-dqn?scriptVersionId=210886670\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport kagglehub\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n!pip install kaggle-environments\n\nfrom kaggle_environments import make\nenv = make(\"connectx\", debug=True)\nenv.reset();","metadata":{"_uuid":"1dae36c0-8c78-43a7-a4e4-54e70975b8a6","_cell_guid":"68646077-4176-4c5a-b8eb-e58eb64fbb77","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:38.293109Z","iopub.execute_input":"2024-12-02T22:22:38.293569Z","iopub.status.idle":"2024-12-02T22:22:48.688684Z","shell.execute_reply.started":"2024-12-02T22:22:38.293533Z","shell.execute_reply":"2024-12-02T22:22:48.68723Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# State representation","metadata":{"_uuid":"94186710-a2a7-4414-8880-60a630818ef8","_cell_guid":"31b01ab7-0d74-4c10-b614-b7ec17264a69","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def get_state(env):\n    '''\n    Given an environment, return the board state.\n    '''\n    return env.state[0]['observation']['board']","metadata":{"_uuid":"46f3bebb-cce3-4c78-bb3e-363c43d9c174","_cell_guid":"b4eba8de-307e-4a9c-9f52-b36c34e7b35a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:48.691144Z","iopub.execute_input":"2024-12-02T22:22:48.691512Z","iopub.status.idle":"2024-12-02T22:22:48.697229Z","shell.execute_reply.started":"2024-12-02T22:22:48.691473Z","shell.execute_reply":"2024-12-02T22:22:48.69621Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Q-network","metadata":{"_uuid":"d5662e2d-92b6-4804-8271-4774d225b3a8","_cell_guid":"811dc025-8f6f-4000-afa2-c89e28f48369","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class NeuralNetwork(nn.Module):\n    '''\n    Neural network for approximating the Bellman equation.\n    '''\n    def __init__(self):\n        super().__init__()\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(42, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 7)\n        \n        # Non-linear activation\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.2)  # Dropout to prevent overfitting\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(self.relu(self.fc2(x)))\n        x = self.relu(self.fc3(x))\n        x = self.fc4(x)  # No activation on the output layer\n        return x","metadata":{"_uuid":"227b19b0-588a-4dcd-a2da-dd5a3aa2209e","_cell_guid":"bb2c3dad-42b7-4fcb-9e5b-a55bdbf8beba","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:48.698529Z","iopub.execute_input":"2024-12-02T22:22:48.698853Z","iopub.status.idle":"2024-12-02T22:22:48.713567Z","shell.execute_reply.started":"2024-12-02T22:22:48.698816Z","shell.execute_reply":"2024-12-02T22:22:48.71254Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Replay buffer","metadata":{"_uuid":"738209e5-5afb-4222-8439-0cce198a2319","_cell_guid":"f05a63f1-0d59-4973-acf1-6323e15bcb38","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from collections import deque\nfrom random import sample\n\nclass ReplayBuffer():\n    '''\n    A class to store experiences: state, action, reward, next_state.\n    '''\n    def __init__(self, capacity):\n        \n        # intialise replay buffer\n        self.buffer = deque()\n        self.capacity = capacity\n        \n    def __len__(self):\n        return len(self.buffer)\n        \n    def append(self, new_experience):\n        '''Append new experience to buffer.'''\n        \n        self.buffer.append(new_experience)\n        \n        if len(self.buffer) > self.capacity: # if buffer reaches maximum size\n            self.buffer.popleft() # pop the first element\n        \n    def sample(self, batch_size):\n        '''Sample a batch of experiences.'''\n        \n        batch = sample(self.buffer, batch_size)\n        \n        return batch","metadata":{"_uuid":"205abe78-6fe3-4184-bb19-7c04d13ee7c1","_cell_guid":"ebd35cf4-8367-4735-acc2-79b6037e50bf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:48.716012Z","iopub.execute_input":"2024-12-02T22:22:48.716521Z","iopub.status.idle":"2024-12-02T22:22:48.730694Z","shell.execute_reply.started":"2024-12-02T22:22:48.716483Z","shell.execute_reply":"2024-12-02T22:22:48.729552Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Agents","metadata":{"_uuid":"9153aa77-3381-4698-925a-81b2fdde6db6","_cell_guid":"0519836f-2a37-427b-bc7c-bb6a22fb145c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# This agent randomly chooses a non-empty column.\ndef random_agent(observation, configuration):\n    from random import choice\n    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])","metadata":{"_uuid":"488cb283-fb75-4629-b32f-0e2ad32d5c7b","_cell_guid":"ad93fa00-bb67-4936-a35f-e5d4828f5d3a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:48.732066Z","iopub.execute_input":"2024-12-02T22:22:48.732421Z","iopub.status.idle":"2024-12-02T22:22:48.747507Z","shell.execute_reply.started":"2024-12-02T22:22:48.732389Z","shell.execute_reply":"2024-12-02T22:22:48.746403Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\n# This agent chooses an action using the neural network\ndef my_agent(observation, configuration):\n    \n    # current state\n    state = observation['board']\n    \n    # get legal moves\n    legal_moves = [0 if observation[\"board\"][column] == 0 else -100000 for column in range(configuration.columns)]\n    \n    # NN prediction on current state\n    y = model.forward(torch.tensor(np.array(state, dtype='float32')))\n\n    # apply legal move masking\n    legal_moves_tensor = torch.tensor(legal_moves)\n    legal_y = y + legal_moves_tensor\n\n    # find highest q-value\n    best_move = torch.argmax(legal_y)\n    q_value = y[best_move]\n\n    return best_move.item()","metadata":{"_uuid":"43c51045-bc5f-4a74-b105-3c3b8922bf9f","_cell_guid":"6d52648d-2cbe-425f-b4f4-396b42a5666f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:48.748878Z","iopub.execute_input":"2024-12-02T22:22:48.749435Z","iopub.status.idle":"2024-12-02T22:22:48.764877Z","shell.execute_reply.started":"2024-12-02T22:22:48.749399Z","shell.execute_reply":"2024-12-02T22:22:48.763781Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def agent_play(matches, rb, epsilon):\n    \"\"\"\n    \n    Function to get the agent playing in the environment for a fixed number of matches.\n    Experiences are added to the replay buffer. Intended for model training. \n    \n    Parameters:\n    \n    matches - number of matches to play\n    rb - replay buffer to store experiences to\n    \n    \"\"\"\n    trainer = env.train([None, \"random\"])\n\n    trainer.reset();\n    \n    results = [] # save results\n    \n    for i in range(matches):\n                \n        while not env.done:\n            \n            # get observation\n            observation = env.state[0]['observation']\n\n            # choose an action\n            if random.uniform(0, 1) < epsilon: # explore\n                action = random.choice([c for c in range(env.configuration.columns) if observation.board[c] == 0])\n            else:\n                action = my_agent(observation, env.configuration)\n\n            # take action and get results\n            new_observation, reward, done, info = trainer.step(action)\n\n            # capture experience\n            experience = (observation['board'], action, reward, new_observation['board'], observation['mark'])\n\n            # append experience to replay buffer\n            rb.append(experience)\n        \n        if i == range(matches)[-1]: # last matches ...\n#             env.render(mode='ipython') # show result\n            print(\"Match {} complete\".format(i))\n        \n        results.append(reward) # append results\n        trainer.reset() # restart the game\n    \n    return results","metadata":{"_uuid":"62bc8b1c-230a-4ea1-822f-3731740db765","_cell_guid":"834e5723-9e22-4714-9e87-408428acb620","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:48.766287Z","iopub.execute_input":"2024-12-02T22:22:48.76666Z","iopub.status.idle":"2024-12-02T22:22:48.786569Z","shell.execute_reply.started":"2024-12-02T22:22:48.766628Z","shell.execute_reply":"2024-12-02T22:22:48.785163Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model training","metadata":{"_uuid":"5d3daa28-1eed-4850-bac9-8ec3d6771afd","_cell_guid":"7c6e34c7-c529-43b9-83fc-51c08c0ca935","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from torch.optim import Adam\n# from torch.optim.lr_scheduler import ExponentialLR\n\ngamma = 0.8 # discount factor\n\ndef train_model(model, optimizer, scheduler, rb, batch_size):\n    \"\"\"\n    \n    Function to train the model. \n    Parameters:\n    \n    model - PyTorch neural network\n    rb - replay buffer\n    sample_size - the number of experiences to train the model with\n    \n    \"\"\"\n    \n    # get sample of experiences\n    batch = rb.sample(batch_size=batch_size)\n\n    states = torch.tensor([experience[0] for experience in batch], dtype=torch.float32)\n    actions = torch.tensor([experience[1] for experience in batch])\n    rewards = torch.tensor([experience[2] for experience in batch])\n    next_states = torch.tensor([experience[3] for experience in batch], dtype=torch.float32)\n    \n    # generate more complex rewards\n    rewards_extra = get_rewards(rewards)\n    \n    # NN predictions on current states\n    y = model.forward(states)\n    q_values = y.gather(1, actions.unsqueeze(1)).squeeze()\n\n    # NN prediction on next state\n    y_next_state = model.forward(next_states)\n    q_values_next_state = y_next_state.max(dim=1)[0].detach()\n\n    # bellman equation\n    q_targets = rewards_extra + gamma*q_values_next_state.detach()\n\n    # loss\n    loss = ((q_targets - q_values)**2).mean()\n    print(\"Loss:\", loss.item())\n\n    optimizer.zero_grad()\n    loss.backward() # compute gradients\n    optimizer.step() # update weights\n    # scheduler.step() # reduce learning rate\n    \n    return model, loss.item()","metadata":{"_uuid":"1d02c1aa-0ea9-4dda-8797-7d7f2226c13c","_cell_guid":"977fb4d0-6587-49a7-9f84-21e0d02ecbd5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:48.788168Z","iopub.execute_input":"2024-12-02T22:22:48.788526Z","iopub.status.idle":"2024-12-02T22:22:48.808259Z","shell.execute_reply.started":"2024-12-02T22:22:48.788492Z","shell.execute_reply":"2024-12-02T22:22:48.807122Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_rewards(rewards):\n    \"\"\"\n    \n    Function to get more sophisticated rewards from the default ones.\n    Parameters:\n    \n    rewards - tensor of the existing rewards\n    \n    \"\"\"\n    new_rewards = []\n    rewards = [reward.item() for reward in rewards]\n    \n    for reward in rewards:\n        \n        if reward == 1: # agent wins\n            new_reward = 2\n        elif reward == 0: # agent makes a legal move\n            new_reward = 1/42\n        else: # agent loses the match\n            new_reward = -5\n            \n        new_rewards.append(new_reward)\n    \n    return torch.tensor(new_rewards)","metadata":{"_uuid":"f836967a-ea2e-4af8-ab75-f2178eca0d3c","_cell_guid":"921b8ff6-4ef1-47a6-90a2-62e346ef7d9f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:48.809518Z","iopub.execute_input":"2024-12-02T22:22:48.809863Z","iopub.status.idle":"2024-12-02T22:22:48.825556Z","shell.execute_reply.started":"2024-12-02T22:22:48.80983Z","shell.execute_reply":"2024-12-02T22:22:48.82438Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main(model, rb, agent, episodes, matches, batch_size, epsilon, learning_rate):\n    \"\"\"\n    Main function to get the agent to play multiple matches of Connect 4 and then train based on it's experiences. \n    Parameters:\n    \n    model - Neural Network\n    rb - Replay Buffer\n    agent - the agent to be trained\n    episodes - how many training episodes to run\n    matches - number of matches to play in each episode\n    batch_size - number of experiences to sample from the replay buffer\n    epsilon - the starting exploration rate\n    learning_rate - the learning rate for the network's optimizer\n    \n    \"\"\"\n    epsilon_discount = epsilon/episodes # amount to reduce epsilon by every episode\n    \n    # Initialize optimizer and scheduler\n    optimizer = Adam(model.parameters(), lr=learning_rate)\n    # scheduler = ExponentialLR(optimizer, gamma=0.95)  # Decay LR by 5% every episode\n    \n    metric_wr = {}\n    metric_loss = {}\n    for i in range(episodes):\n            \n         # save model - untrained, mid-trained, fully-trained\n        if i == 0:\n            print(\"Saving untrained model\", model.state_dict())\n            torch.save(model.state_dict(), \"connect_four_model_untrained_rdm.pth\")\n            \n        elif i == episodes/2:\n            print(\"Saving mid-trained model\", model.state_dict())\n            torch.save(model.state_dict(), \"connect_four_model_mid_trained_rdm.pth\")\n            \n        elif i == (episodes-1):\n            print(\"Saving fully trained model\", model.state_dict())\n            torch.save(model.state_dict(), \"connect_four_model_fully_trained_rdm.pth\")\n        \n        # play\n        results = agent_play(matches=matches, rb=rb, epsilon=epsilon)\n\n        # learn\n        model, loss = train_model(model, optimizer=optimizer, scheduler=None, rb=rb, batch_size=batch_size)\n        \n        # metrics\n        wr = round(results.count(1) / len(results), 2)*100\n        metric_wr[\"Episode {} WR\".format(i)] = wr\n        metric_loss[\"Loss {} WR\".format(i)] = loss\n        print(\"Episode {} WR\".format(i), wr)\n        \n        # reduce gamma for the next episode - increase exploitation\n        epsilon = epsilon - epsilon_discount\n        \n    return metric_wr, metric_loss","metadata":{"_uuid":"52d5ba84-aea1-4901-94ca-14d418bef8a9","_cell_guid":"a436323b-0a89-4080-9e3f-a999dce6a1d5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:48.828641Z","iopub.execute_input":"2024-12-02T22:22:48.828992Z","iopub.status.idle":"2024-12-02T22:22:48.848271Z","shell.execute_reply.started":"2024-12-02T22:22:48.828957Z","shell.execute_reply":"2024-12-02T22:22:48.847181Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model(new=False):\n    \"\"\"\n    \n    Function to get the neural network model. \n    It can create one from scratch or retrieve the latest pre-trained version.\n    \n    Parameters:\n    \n    new - True if new model requested, False otherwise.\n    \n    \"\"\"\n    if new==True:\n        model = NeuralNetwork()\n    else:\n        model = NeuralNetwork()\n        model.load_state_dict(torch.load(\"/kaggle/input/derek/pytorch/random-trained/1/connect_four_model_fully_trained.pth\"))\n        model.eval()\n        \n    return model","metadata":{"_uuid":"105d1c37-f4e8-4f70-a13e-560a375d9f81","_cell_guid":"9468a5a7-a80f-4fe0-8ff0-28d8632def90","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:48.849693Z","iopub.execute_input":"2024-12-02T22:22:48.850137Z","iopub.status.idle":"2024-12-02T22:22:48.865661Z","shell.execute_reply.started":"2024-12-02T22:22:48.850091Z","shell.execute_reply":"2024-12-02T22:22:48.864514Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"derek_pytorch_random_trained_1_path = kagglehub.model_download('jollychappies/derek/PyTorch/random-trained/1')","metadata":{"_uuid":"0a5677d8-e915-4fc0-b1bb-b58b1f371b64","_cell_guid":"9c79550f-3c10-4f7a-b106-d682b4f9a609","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:48.867279Z","iopub.execute_input":"2024-12-02T22:22:48.867743Z","iopub.status.idle":"2024-12-02T22:22:49.809179Z","shell.execute_reply.started":"2024-12-02T22:22:48.867697Z","shell.execute_reply":"2024-12-02T22:22:49.808253Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = get_model(new=True)\nrb = ReplayBuffer(capacity = 10000)","metadata":{"_uuid":"2ef3782b-0043-4734-b5e9-1133e10ca725","_cell_guid":"7c940881-075f-4146-ad7e-80c018cc8b64","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:49.810733Z","iopub.execute_input":"2024-12-02T22:22:49.811165Z","iopub.status.idle":"2024-12-02T22:22:49.825599Z","shell.execute_reply.started":"2024-12-02T22:22:49.811117Z","shell.execute_reply":"2024-12-02T22:22:49.824456Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metric_wr, metric_loss = main(model, rb, agent=my_agent, episodes=3000, matches=20, batch_size=32, epsilon=1, learning_rate=0.001)","metadata":{"_uuid":"90505c42-5f1e-41ab-a26d-5938cdafeb5f","_cell_guid":"68181561-9be8-4fe3-9dd4-f6c8e81619ec","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:22:49.826715Z","iopub.execute_input":"2024-12-02T22:22:49.827027Z","iopub.status.idle":"2024-12-02T22:55:54.994043Z","shell.execute_reply.started":"2024-12-02T22:22:49.826994Z","shell.execute_reply":"2024-12-02T22:55:54.992978Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef evaluate(metrics):\n    \"\"\"\n    Function to visualise learning. \n    \n    Parameters:\n    \n    metrics - the win rates from training episodes.\n    \n    \"\"\"\n    \n    x = np.array(list(metrics.keys()))\n    y = np.array(list(metrics.values()))\n    rolling_avg = pd.DataFrame(y, x, columns = ['win_rate']).rolling(window=10, center=True).mean()\n    \n    figure, axes = plt.subplots(figsize=(10, 7))\n    \n    \n    \n    axes.plot(rolling_avg)\n    axes.set_ylabel(\"Win rate %\")\n    axes.set_xlabel(\"Training episode\")\n    axes.tick_params(axis='x', rotation=70)\n        \n    axes.xaxis.set_major_locator(plt.MaxNLocator(len(x)/50))\n    \n    return figure","metadata":{"_uuid":"9413252f-eda7-4e08-a881-442f30e574c2","_cell_guid":"61cce313-4c91-42e9-858f-de9f67d37d8d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-02T22:55:54.99551Z","iopub.execute_input":"2024-12-02T22:55:54.995868Z","iopub.status.idle":"2024-12-02T22:55:55.004166Z","shell.execute_reply.started":"2024-12-02T22:55:54.995834Z","shell.execute_reply":"2024-12-02T22:55:55.002769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"figure = evaluate(metric_wr)\nfigure.savefig(\"training.png\")","metadata":{"_uuid":"b81adaa1-881a-4249-8917-2df9f15fd8e2","_cell_guid":"d45c75d7-980c-427a-8438-50726488070b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T22:55:55.005597Z","iopub.execute_input":"2024-12-02T22:55:55.005989Z","iopub.status.idle":"2024-12-02T22:55:55.59994Z","shell.execute_reply.started":"2024-12-02T22:55:55.005932Z","shell.execute_reply":"2024-12-02T22:55:55.598684Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}